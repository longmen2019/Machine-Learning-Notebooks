{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68af8c95-110c-459f-b6c2-0527b5345d77",
   "metadata": {},
   "source": [
    "\r\n",
    "### CountVectorizer\r\n",
    "\r\n",
    "`CountVectorizer` converts a collection of documents into a matrix of token counts. This transformation allows the text data to be used with machine learning models. Essentially, it counts the number of times each word appears in the documentoject?.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61033178-ea82-4d0b-9b9f-f6b0bb73474b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 1 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 1 1]\n",
      " [1 1 0 1 1 1 1 0 1 0 0]]\n",
      "{'mayur': 4, 'is': 3, 'nice': 7, 'boy': 2, 'rocks': 9, 'wohoo': 10, 'my': 5, 'name': 6, 'and': 1, 'am': 0, 'pythonista': 8}\n"
     ]
    }
   ],
   "source": [
    "# Import the CountVectorizer class from the sklearn.feature_extraction.text module\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample documents\n",
    "docs = [\"Mayur is a nice boy.\", \"Mayur rocks! Wohoo\", \"My name is Mayur, and I am a Pythonista!\"]\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "x = cv.fit_transform(docs)\n",
    "\n",
    "# Print the dense representation of the matrix\n",
    "print(x.todense())\n",
    "\n",
    "# Print the vocabulary\n",
    "print(cv.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7eaacb-3a98-45a1-bf48-8ca0a2207852",
   "metadata": {},
   "source": [
    "## DictVectorizer\r\n",
    "\r\n",
    "`DictVectorizer` converts mappings to vectors.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "008c5ef9-98a5-4d13-914b-dd7ab260846a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 2. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 1. 0. 2. 1. 2. 0. 3. 3.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# Sample documents as dictionaries\n",
    "docs = [\n",
    "    {\"Mayur\": 1, \"is\": 1, \"awesome\": 2},\n",
    "    {\"No\": 1, \"I\": 1, \"dont\": 2, \"wanna\": 3, \"fall\": 1, \"in\": 2, \"love\": 3}\n",
    "]\n",
    "\n",
    "# Initialize the DictVectorizer\n",
    "dv = DictVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "x = dv.fit_transform(docs)\n",
    "\n",
    "# Print the dense representation of the transformed documents\n",
    "print(x.todense())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3401edf9-7625-474f-b543-fc170a36341b",
   "metadata": {},
   "source": [
    "### TfidfVectorizer\n",
    "In many text analytics applications, we need to convert text into vectors to use with Machine Learning algorithms. This is known as the Vector Space Model. While `CountVectorizer` could be a solution, words like \"the\", \"a\", \"in\", etc., are common and often used in all kinds of documents. Using `CountVectorizer` gives more emphasis on such word counts, which are not always relevant. You could circumvent this problem using `stop_words=\"english\"` to filter out common words, but let's say you have a different vocabulary. For instance, a conversation between two Computer Science students might frequently include words like \"RAM\", \"processor\", \"GPU\", and you'd have to manually add these stop words every time for all the problems you solve.\n",
    "\n",
    "In such scenarios, it is recommended to use `TfidfVectorizer`, which takes care of these issues. Every word is given a number according to the following formula:\n",
    "\n",
    "$$\n",
    "\\text{tfidf}(\\text{word}) = \\text{tf}(\\text{word}, \\text{document}_i) \\cdot \\text{idf}(\\text{word})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "1. **tf(word, document_i)** = Term Frequency of a word in the specific document \\(i\\).\n",
    "2. **idf(word)** = Inverse Document Frequency of the word.\n",
    "\n",
    "Inverse Document Frequency is defined as the log of the ratio of the number of documents to the number of times the word has occurred in any document:\n",
    "\n",
    "$$\n",
    "\\text{idf}(w) = \\log\\left(\\frac{n_d}{df(w)}\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "1. **df(w)** = number of times the word has occurred in any document.\n",
    "\n",
    "Intuitively, if a word has occurred too many times in other documents as well (common words like \"the\", \"is\"), it gives lesser weight to such words in contrast to words that have occurred more frequently in a single document compared to others. This means that if a particular word occurs more frequently in a single document only, it might be an important feature.\n",
    "\n",
    "Note that the numerator and denominator are added with `1` to avoid underflow, e.g., when the document frequency is 0.\n",
    "\n",
    "`sklearn` additionally normalizes the output of `tfidf` to have a norm of 1. This is important since we're interested in similarities; hence vectors like (1,1) and (3,3) are really the same (they go in the same direction, just have different magnitudes). This is achieved by dividing by the length of the vector:\n",
    "\n",
    "$$\n",
    "v_i = \\frac{v_i}{|v|_2} = \\frac{v_i}{\\sqrt{v_1^2 + v_2^2 + v_3^2 + \\ldots + v_n^2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d451cf66-c887-4332-8cd4-3e3298b68b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.76749457 0.45329466 0.45329466 0.         0.        ]\n",
      " [0.         0.         0.45329466 0.45329466 0.76749457 0.        ]\n",
      " [0.6088451  0.         0.35959372 0.35959372 0.         0.6088451 ]]\n",
      "{'mayur': 3, 'is': 2, 'guitarist': 1, 'musician': 4, 'also': 0, 'programmer': 5}\n",
      "[[0 1 1 1 0 0]\n",
      " [0 0 1 1 1 0]\n",
      " [1 0 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Initialize TfidfVectorizer and CountVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "cv_vectorizer = CountVectorizer()\n",
    "\n",
    "# Sample documents\n",
    "docs = [\n",
    "    \"Mayur is a Guitarist\",\n",
    "    \"Mayur is a Musician\",\n",
    "    \"Mayur is also a programmer\"\n",
    "]\n",
    "\n",
    "# Fit and transform the documents using TfidfVectorizer\n",
    "x_idf = tfidf_vectorizer.fit_transform(docs)\n",
    "\n",
    "# Fit and transform the documents using CountVectorizer\n",
    "x_cv = cv_vectorizer.fit_transform(docs)\n",
    "\n",
    "# Print the dense representation of the TF-IDF transformed documents\n",
    "print(x_idf.todense())\n",
    "\n",
    "# Print the vocabulary learned by TfidfVectorizer\n",
    "print(tfidf_vectorizer.vocabulary_)\n",
    "\n",
    "# Print the dense representation of the CountVectorizer transformed documents\n",
    "print(x_cv.todense())\n",
    "\n",
    "print(\"We can se\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d036e5-dc6d-44b8-b470-478bc97dd255",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
